{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9449fd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general packages and libraries\n",
    "import os\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import importlib\n",
    "\n",
    "# data manipulation packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# data visualizations packages\n",
    "import matplotlib.pyplot as plt\n",
    "# to render plots in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "# set a theme for seaborn\n",
    "sns.set_theme()\n",
    "\n",
    "# numerical, statistical and machine learning packages and libraries\n",
    "import xgboost as xgb\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn import (\n",
    "    ensemble,\n",
    "    tree,\n",
    ")\n",
    "from sklearn.base import (\n",
    "    BaseEstimator, \n",
    "    TransformerMixin,\n",
    ")\n",
    "from sklearn.pipeline import (\n",
    "    make_pipeline,\n",
    "    FeatureUnion, \n",
    "    Pipeline,\n",
    ")\n",
    "from sklearn.feature_selection import (\n",
    "    SelectKBest, \n",
    "    chi2, \n",
    "    mutual_info_classif,\n",
    "    f_classif,\n",
    ")\n",
    "from sklearn.impute import (\n",
    "    KNNImputer,\n",
    "    SimpleImputer,\n",
    ")\n",
    "from sklearn.preprocessing import (\n",
    "    OneHotEncoder, \n",
    "    OrdinalEncoder, \n",
    "    LabelEncoder,\n",
    "    StandardScaler,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    StratifiedKFold,\n",
    "    KFold,\n",
    "    cross_val_score,\n",
    ")\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "#from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "#from sklearn.ensemble import (RandomForestClassifier)\n",
    "\n",
    "from sklearn.linear_model import (\n",
    "    SGDClassifier,\n",
    "    LogisticRegression,\n",
    ") \n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    r2_score, \n",
    "    mean_squared_error,\n",
    "    auc,\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7993e6b6",
   "metadata": {},
   "source": [
    "# Analysis of StackOverflow Survey\n",
    "\n",
    "In this notebook we collect several useful tidbits developed or collected during the analysis of the StackOverflow Survey. The examples are built on the survey data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb9ad29e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64461, 61)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a path string\n",
    "mypath = os.getcwd()\n",
    "\n",
    "# upload the datafiles as pandas dataframes\n",
    "df1 = pd.read_csv(mypath+'/data/survey20_updated.csv', index_col=[0])\n",
    "\n",
    "# check the uploaded data\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cf7b63",
   "metadata": {},
   "source": [
    "## Feature selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6125b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/Chancylin/StackOverflow_Survey/blob/main/code/data_process.py\n",
    "\n",
    "def cal_mutual_info(df, target_var=None, disc_features_only=True):\n",
    "    \"\"\"Calculate mutual information for feature selection, based on mutual_info_classif from sklearn.feature_selection.\n",
    "    :param df: Pandas dataframe\n",
    "    :param target_var: target variable\n",
    "    :param disc_features_only: boolean, calculate mutual information for discrete feature only\n",
    "    :return:\n",
    "        a Pandas dataframe with mutual information for features\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    df_f_type = df.dtypes\n",
    "    df_f_type = df_f_type.loc[~df_f_type.index.isin([target_var])].copy()\n",
    "    cols_if_num = df_f_type.apply(lambda x: np.issubdtype(x, np.number))\n",
    "    discrete_f = ~cols_if_num\n",
    "    # get all categorical features\n",
    "    cols_num = cols_if_num[cols_if_num].index.tolist()\n",
    "    cols_cat = cols_if_num[~cols_if_num].index.tolist()\n",
    "\n",
    "    for col_cat in cols_cat:\n",
    "        df[col_cat] = df[col_cat].fillna('Missing')\n",
    "\n",
    "    for col_num in cols_num:\n",
    "        df[col_num] = df[col_num].fillna(df[col_num].mean())\n",
    "        \n",
    "    enc = OrdinalEncoder()\n",
    "    df[cols_cat] = enc.fit_transform(df[cols_cat])\n",
    "    enc = OrdinalEncoder()\n",
    "    df.loc[:, target_var] = enc.fit_transform(df[[target_var]])\n",
    "\n",
    "    if not disc_features_only:\n",
    "        all_features = df_f_type.index.tolist()\n",
    "        mutual_info = mutual_info_classif(df[all_features], df[target_var].values,\n",
    "                                          discrete_features=discrete_f,\n",
    "                                          n_neighbors=20,\n",
    "                                          random_state=123)\n",
    "        df_mutual_info = pd.DataFrame(data=zip(all_features, mutual_info), columns=['columns', 'mutual_info'])\n",
    "        return df_mutual_info\n",
    "    else:\n",
    "\n",
    "        mutual_info = mutual_info_classif(df[cols_cat], df[target_var].values,\n",
    "                                          discrete_features=True)\n",
    "        df_mutual_info = pd.DataFrame(data=zip(cols_cat, mutual_info), columns=['columns', 'mutual_info'])\n",
    "        return df_mutual_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4af4a0",
   "metadata": {},
   "source": [
    "### My approach: step_by_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "719e41b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Age', 'CompTotal', 'ConvertedComp', 'WorkWeekHrs']\n"
     ]
    }
   ],
   "source": [
    "# the list of numerical columns\n",
    "num_cols = df1.select_dtypes(include='float64').columns.to_list()\n",
    "print(num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55f91054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MainBranch', 'Hobbyist', 'Age1stCode', 'CompFreq', 'Country', 'CurrencyDesc', 'CurrencySymbol', 'DatabaseDesireNextYear', 'DatabaseWorkedWith', 'DevType', 'EdLevel', 'Employment', 'Ethnicity', 'Gender', 'JobFactors', 'JobSat', 'JobSeek', 'LanguageDesireNextYear', 'LanguageWorkedWith', 'MiscTechDesireNextYear', 'MiscTechWorkedWith', 'CollabToolsDesireNextYear', 'CollabToolsWorkedWith', 'DevOps', 'DevOpsImpt', 'EdImpt', 'JobHunt', 'JobHuntResearch', 'Learn', 'OffTopic', 'OnboardGood', 'OtherComms', 'Overtime', 'PurchaseResearch', 'PurpleLink', 'SOSites', 'Stuck', 'OpSys', 'OrgSize', 'PlatformDesireNextYear', 'PlatformWorkedWith', 'PurchaseWhat', 'Sexuality', 'SOAccount', 'SOComm', 'SOPartFreq', 'SOVisitFreq', 'SurveyEase', 'SurveyLength', 'Trans', 'UndergradMajor', 'WebframeDesireNextYear', 'WebframeWorkedWith', 'WelcomeChange', 'YearsCode', 'YearsCodePro', 'DevClass']\n"
     ]
    }
   ],
   "source": [
    "# the list of categorical columns, drop the target 'JobSat'\n",
    "cat_cols = df1.select_dtypes(include='object').columns.to_list()\n",
    "print(cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8cca2664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the list of columns with high cardinality - will get a high selection score\n",
    "multiple = ['DatabaseDesireNextYear', 'DatabaseWorkedWith', \n",
    "           'LanguageDesireNextYear', 'LanguageWorkedWith',\n",
    "            'MiscTechDesireNextYear', 'MiscTechWorkedWith', \n",
    "            'CollabToolsDesireNextYear', 'CollabToolsWorkedWith', \n",
    "            'PlatformDesireNextYear', 'PlatformWorkedWith', \n",
    "            'WebframeDesireNextYear', 'WebframeWorkedWith']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad532941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Employment', 'Stuck', 'Age1stCode', 'UndergradMajor', 'YearsCodePro', 'Sexuality', 'Ethnicity', 'EdImpt', 'Gender', 'JobSeek', 'DevOpsImpt', 'Trans', 'DevType', 'OpSys', 'Overtime', 'JobFactors', 'OtherComms', 'SOVisitFreq', 'Hobbyist', 'SurveyEase', 'YearsCode', 'PurchaseWhat', 'PurpleLink', 'PurchaseResearch', 'SOSites', 'JobHunt', 'WelcomeChange', 'MainBranch', 'SurveyLength', 'Country', 'OnboardGood', 'SOComm', 'DevClass', 'DevOps', 'JobHuntResearch', 'OrgSize', 'CompFreq', 'EdLevel', 'CurrencyDesc', 'OffTopic', 'SOAccount', 'CurrencySymbol', 'SOPartFreq', 'Learn']\n"
     ]
    }
   ],
   "source": [
    "# remove target 'JobSat' and the multiple columns from cat_cols\n",
    "cat_cols = list(set(cat_cols) - set(multiple))\n",
    "cat_cols.remove('JobSat')\n",
    "print(cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8354b521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop all missing entries in 'JobSat'\n",
    "df1.dropna(subset=['JobSat'], inplace=True)\n",
    "# check output\n",
    "df1['JobSat'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3af85dd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MainBranch          0\n",
       "Hobbyist            0\n",
       "Age              8839\n",
       "Age1stCode          0\n",
       "CompFreq            0\n",
       "                 ... \n",
       "WelcomeChange       0\n",
       "WorkWeekHrs      4137\n",
       "YearsCode           0\n",
       "YearsCodePro        0\n",
       "DevClass            0\n",
       "Length: 61, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fill in 'missing' in all categorical columns in the list\n",
    "for col in cat_cols:\n",
    "        df1[col] = df1[col].fillna('missing')\n",
    "# check outcome\n",
    "df1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "00aebee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age              0\n",
       "CompTotal        0\n",
       "ConvertedComp    0\n",
       "WorkWeekHrs      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fill in 'median' in all categorical columns in the list\n",
    "for col in num_cols:\n",
    "        df1[col] = df1[col].fillna(df1[col].median())\n",
    "# check outcome\n",
    "df1[num_cols].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "128e8053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the numerical columns\n",
    "\n",
    "# create an instance of the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# scale the numerical variables, fit and transform on the straining set\n",
    "df1[num_cols] = pd.DataFrame(scaler.fit_transform(df1[num_cols]), \n",
    "                                columns=df1[num_cols].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "6234f9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the categorical variables\n",
    "enc = OrdinalEncoder()\n",
    "df1[cat_cols] = enc.fit_transform(df1[cat_cols])\n",
    "enc = OrdinalEncoder()\n",
    "df1.loc[:, 'JobSat'] = enc.fit_transform(df1[['JobSat']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d21265f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection function\n",
    "# to be rewritten as a custom transformer class\n",
    "def select_features(X_train, y_train, score_function, kval, no_cols): # add X_test\n",
    "    \"\"\"\n",
    "    Function for feature selection for (discrete) variables.\n",
    "    INPUT: X_train - input dataframe, must have only discrete or only continuous features, \n",
    "                     pre-processed by removing/imputing missing values and encoded\n",
    "           y_train - target pd.series, pre-processed\n",
    "           score_function - can be any of the score functions supported by SelectKBest\n",
    "           kval = the number of best features to return, can be 'all'\n",
    "           no_cols = number of columns to print\n",
    "    OUTPUT: dataframe with two columns, one for the column names in X_train, \n",
    "            the second for the scores computed, sorted in decreasing order of the scores\n",
    "            #variant: if transform of X_test set is performed, \n",
    "                      it also returns the transformed dataframe\n",
    "    \"\"\"\n",
    "    # create an instance of the selector\n",
    "    fs = SelectKBest(score_func=score_function, k=kval)\n",
    "    # fit the selector on the train set and the train target values\n",
    "    fs.fit(X_train, y_train)\n",
    "    # transform the train set, it will have only the kbest columns\n",
    "    X_train_r = fs.transform(X_train) \n",
    "    # transform the test set, it will have only the kbest columns\n",
    "    # X_test_r = fs.transform(X_test) \n",
    "    # get column names for kbest columns\n",
    "    cols_info = fs.get_support(indices=True)\n",
    "    cols = X_train.iloc[:,cols_info].columns\n",
    "    # put columns and their scores together in a dataframe\n",
    "    frame_best = pd.DataFrame(data=zip(cols,fs.scores_), columns = ['cat_columns', 'kbest_scores'])\n",
    "    return frame_best.sort_values(by='kbest_scores', ascending=False).head(no_cols)\n",
    "    #return X_r, frame_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "40007dd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_columns</th>\n",
       "      <th>kbest_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>JobSeek</td>\n",
       "      <td>0.095091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>OnboardGood</td>\n",
       "      <td>0.031250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>JobHunt</td>\n",
       "      <td>0.025677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>CurrencyDesc</td>\n",
       "      <td>0.024917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Country</td>\n",
       "      <td>0.022552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>CurrencySymbol</td>\n",
       "      <td>0.016086</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cat_columns  kbest_scores\n",
       "9          JobSeek      0.095091\n",
       "30     OnboardGood      0.031250\n",
       "25         JobHunt      0.025677\n",
       "38    CurrencyDesc      0.024917\n",
       "29         Country      0.022552\n",
       "41  CurrencySymbol      0.016086"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_features(df1[cat_cols], df1['JobSat'], mutual_info_classif, 'all', 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "cbeadca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternate approach, using mutual_info_classif\n",
    "mutual_info = mutual_info_classif(df1[cat_cols], df1['JobSat'],\n",
    "                                          discrete_features='auto',\n",
    "                                          n_neighbors=3,\n",
    "                                          copy=True,\n",
    "                                          random_state=42)\n",
    "df_mutual_info = pd.DataFrame(data=zip(cat_cols, mutual_info), columns=['columns', 'mutual_info'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96aeab0e",
   "metadata": {},
   "source": [
    "## Discretise continuous variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badc8282",
   "metadata": {},
   "source": [
    "### Create bins for the WorkWeekHrs column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38e9f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the labels\n",
    "cut_labels = ['less-10', '10-20', '20-30', '30-40', '40-50', 'more-50']\n",
    "\n",
    "# define the bins \n",
    "m = df1.WorkWeekHrs.max()\n",
    "cut_bins = [0, 10, 20, 30, 40, 50, m]\n",
    "\n",
    "# create a new column which contains the new labels\n",
    "df1['WorkWeek_Bins'] = pd.cut(df1['WorkWeekHrs'], bins=cut_bins, labels=cut_labels)\n",
    "\n",
    "# check for success\n",
    "df1['WorkWeek_Bins'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfe3260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the type of the newly created column\n",
    "df1['WorkWeek_Bins'] = df1['WorkWeek_Bins'].astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622851d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the WorkWeekHrs column\n",
    "df1.drop(columns = 'WorkWeekHrs', inplace=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7360c337",
   "metadata": {},
   "source": [
    "### Create bins for the ConvertedComp column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06f88ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we could use quantile, however I prefer custom bins here\n",
    "cut_labels = ['less-10K', '10K-30K', '30K-50K', '50K-100K', '100K-200K', 'more-200K']\n",
    "\n",
    "# define the bins \n",
    "m = df1.ConvertedComp.max()\n",
    "cut_bins = [0, 10000, 30000, 50000, 100000, 200000, m]\n",
    "\n",
    "# create a new column which contains the new labels\n",
    "df1['Comp_Bins'] = pd.cut(df1['ConvertedComp'], bins=cut_bins, labels=cut_labels)\n",
    "\n",
    "# change the type of the newly created column\n",
    "df1['Comp_Bins'] = df1['Comp_Bins'].astype('object')\n",
    "\n",
    "# drop the WorkWeekHrs column\n",
    "df1.drop(columns = 'ConvertedComp', inplace=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f30ade",
   "metadata": {},
   "source": [
    "### Create bins for the Age column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6297d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the bin edges\n",
    "cut_labels = ['<20', '20-30', '30-40', '40-50', '50-60', '60-70', '70-80', '>80']\n",
    "\n",
    "# define the bins \n",
    "m = df1.Age.max()\n",
    "cut_bins = [0, 20, 30, 40, 50, 60, 70, 80, m]\n",
    "\n",
    "# create a new column which contains the new labels\n",
    "df1['Age_Bins'] = pd.cut(df1['Age'], bins=cut_bins, labels=cut_labels)\n",
    "\n",
    "# change the type of the newly created column\n",
    "df1['Age_Bins'] = df1['Age_Bins'].astype('object')\n",
    "\n",
    "# drop the WorkWeekHrs column\n",
    "df1.drop(columns = 'Age', inplace=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff817994",
   "metadata": {},
   "source": [
    "### Remove the rows that contain mostly missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e323f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the rows with at least 10 non-NA values\n",
    "df1.dropna(thresh=10)\n",
    "\n",
    "# check the result\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6243357a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.JobSat.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6c1915",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.dropna( how='any', subset=['JobSat'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9685410",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.JobSat.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad0c14a",
   "metadata": {},
   "source": [
    "### Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c883ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate rows, if any\n",
    "df1.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7b99f9",
   "metadata": {},
   "source": [
    "## Create features and target\n",
    "\n",
    "Create a dataframe (X) with the features and a pandas series (y) that contains the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b40b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy of the pre-processed dataframe\n",
    "df2 = df1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf816df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the predictors dataframe\n",
    "X = df2.drop(columns = 'JobSat')\n",
    "\n",
    "# create the labels\n",
    "y = df2['JobSat']\n",
    "\n",
    "# check for success\n",
    "X.info(), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c049e8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# format all the fields as strings in the feature matrix\n",
    "X = X.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a45ce76",
   "metadata": {},
   "source": [
    "## Sample data\n",
    "\n",
    "We will use $30 \\%$ data for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0d97b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data in train and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# summarize the data\n",
    "print('Train', X_train.shape, y_train.shape)\n",
    "print('Test', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a115bc",
   "metadata": {},
   "source": [
    "## Impute missing values\n",
    "\n",
    "Now that we have test and train data, we can impute missing values on the training set, and use the trained imputer to fill in the test dataset. I will use the KNN imputer from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4945f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of the imputer\n",
    "#imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "# fit the imputer on the dataset\n",
    "#X_train_trans = pd.DataFrame(imputer.fit_transform(X_train), columns = X_train.columns)\n",
    "\n",
    "# check for success\n",
    "#X_train_trans.isna().any()\n",
    "from sklearn.impute import SimpleImputer\n",
    "def impute_predictors(X_train, X_test):\n",
    "    imputer = SimpleImputer(strategy='constant', fill_value='missing')\n",
    "    imputer.fit(X_train)\n",
    "    X_train_trans = pd.DataFrame(imputer.transform(X_train), columns=X_train.columns)\n",
    "    X_test_trans = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)\n",
    "    return X_train_trans, X_test_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483501ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_trans, X_test_trans = impute_predictors(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4962e5b1",
   "metadata": {},
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, [0])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c2de48",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_trans.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa411ff",
   "metadata": {},
   "source": [
    "## Encode the data\n",
    "\n",
    "The best practice when encoding variables is to fit the encoding on the training dataset, then apply it to the train and test datasets.\n",
    "\n",
    "The function below named prepare_inputs() takes the input data for the train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16463bad",
   "metadata": {},
   "outputs": [],
   "source": [
    " # prepare input data\n",
    "def encode_predictors(X_train, X_test):\n",
    "\tenc = OneHotEncoder(handle_unknown='ignore')\n",
    "\tenc.fit(X_train)\n",
    "\tX_train_enc = enc.transform(X_train)\n",
    "\tX_test_enc = enc.transform(X_test)\n",
    "\treturn X_train_enc, X_test_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96179add",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_enc, X_test_enc = encode_predictors(X_train_trans, X_test_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d614018",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19108de7",
   "metadata": {},
   "source": [
    "Regarding the output data, the target, since it is already encoded as an integer with values from 0 to 5, no other encoding steps are needed at this point.\n",
    "\n",
    "Alternative would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c6ae2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare target\n",
    "#def encode_targets(y_train, y_test):\n",
    "\t#le = LabelEncoder()\n",
    "\t#le.fit(y_train)\n",
    "\t#y_train_enc = le.transform(y_train)\n",
    "\t#y_test_enc = le.transform(y_test)\n",
    "\t#return y_train_enc, y_test_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97eb6d1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0bd6ebe",
   "metadata": {},
   "source": [
    "## Refactor code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17aef62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c06092e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a path string\n",
    "mypath = os.getcwd()\n",
    "\n",
    "# read the data from the file\n",
    "df = pd.read_csv(mypath+'/data/survey20_updated.csv')\n",
    "# preprocess, split and process data\n",
    "preproc_df = uf.preprocess_data(df)\n",
    "X_train, y_train, X_test, y_test = uf.process_data(preproc_df, 'JobSat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d47cc4",
   "metadata": {},
   "source": [
    "## Baseline model: K NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9934c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of the classifier\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# fit the classifier\n",
    "knn_clf.fit(X_train_fs, y_train)\n",
    "\n",
    "# predict output values\n",
    "y_pred = knn_clf.predict(X_test_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c3bf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print evaluation metrics and results\n",
    "\n",
    "result1 = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(result1)\n",
    "\n",
    "result2 = classification_report(y_test, y_pred)\n",
    "print('\\nClassification Report:')\n",
    "print (result2)\n",
    "\n",
    "result3 = accuracy_score(y_test,y_pred)  \n",
    "print('Accuracy: %.3f' %result3)\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8cd09a",
   "metadata": {},
   "source": [
    "## Several other algorithms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f664a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# create classifier instance\n",
    "svm_clf = SVC(gamma=\"auto\", random_state=42)\n",
    "# fit the model\n",
    "svm_clf.fit(X_train_fs, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0f5392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on the test set\n",
    "y_pred = svm_clf.predict(X_test_fs)\n",
    "\n",
    "# test one value\n",
    "y_test.iloc[20],  y_pred[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae437bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_digit_scores = svm_clf.decision_function(X_test_fs)\n",
    "some_digit_scores[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd7f7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(some_digit_scores[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acae03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376bba3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173c0f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(svm_clf, X_test_fs, y_test, cv=10, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0d8b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print evaluation metrics and results\n",
    "\n",
    "result1 = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(result1)\n",
    "\n",
    "result2 = classification_report(y_test, y_pred, zero_division=0)\n",
    "print('\\nClassification Report:')\n",
    "print (result2)\n",
    "\n",
    "result3 = accuracy_score(y_test,y_pred)  \n",
    "print('Accuracy: %.3f' %result3)\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b45d132",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "ovr_clf = OneVsRestClassifier(SVC(gamma=\"auto\", random_state=42))\n",
    "ovr_clf.fit(X_train_fs, y_train)\n",
    "y_pred = ovr_clf.predict(X_test_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac945a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(result1)\n",
    "\n",
    "result2 = classification_report(y_test, y_pred, zero_division=0)\n",
    "print('\\nClassification Report:')\n",
    "print (result2)\n",
    "\n",
    "result3 = accuracy_score(y_test,y_pred)  \n",
    "print('Accuracy: %.3f' %result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f0b71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Always scale the input. The most convenient way is to use a pipeline.\n",
    "sgd_clf = make_pipeline(StandardScaler(with_mean=False),SGDClassifier(max_iter=1000, tol=1e-3))\n",
    "sgd_clf.fit(X_train_fs, y_train)\n",
    "y_pred = sgd_clf.predict(X_test_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3033a0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(result1)\n",
    "\n",
    "result2 = classification_report(y_test, y_pred, zero_division=0)\n",
    "print('\\nClassification Report:')\n",
    "print (result2)\n",
    "\n",
    "result3 = accuracy_score(y_test,y_pred)  \n",
    "print('Accuracy: %.3f' %result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed95eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([X_train_fs, X_test_fs])\n",
    "y = pd.concat([y_train, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23bdba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import (LogisticRegression)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import (KNeighborsClassifier)\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import (RandomForestClassifier)\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d44922",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in [DecisionTreeClassifier, KNeighborsClassifier, GaussianNB, SVC, \n",
    "              RandomForestClassifier, SGDClassifier]:\n",
    "    make_pipeline(StandardScaler(),model())\n",
    "    classifier = model()\n",
    "    kfold = model_selection.KFold(n_splits=10)\n",
    "    classifier.fit(X_train_fs.toarray(), y_train)\n",
    "    s = model_selection.cross_val_score(classifier, X_test_fs.toarray(),y_test, cv=kfold)\n",
    "    #result2 = classification_report(y_test, y_pred, zero_division=0)\n",
    "    #s = model_selection.cross_val_score(cls, X, y, cv=kfold)\n",
    "    print(f\"{model.__name__:22}  CV_Mean:\" f\"{s.mean():.3f} CV_STD: {s.std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91249c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier(objective=\"multi:softprob\", random_state=42)\n",
    "xgb_model.fit(X_train_fs, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e1ab8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb_model.predict(X_test_fs)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ba9767",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6423fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer, load_diabetes, load_wine\n",
    "from sklearn.metrics import auc, accuracy_score, confusion_matrix, mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold, RandomizedSearchCV, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633392f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier(objective=\"multi:softprob\", random_state=42)\n",
    "\n",
    "params = {\n",
    "    \"colsample_bytree\": uniform(0.7, 0.3),\n",
    "    \"gamma\": uniform(0, 0.5),\n",
    "    \"learning_rate\": uniform(0.03, 0.3), # default 0.1 \n",
    "    \"max_depth\": randint(2, 6), # default 3\n",
    "    \"n_estimators\": randint(100, 150), # default 100\n",
    "    \"subsample\": uniform(0.6, 0.4)\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(xgb_model, param_distributions=params, random_state=42, \n",
    "                            n_iter=200, cv=3, verbose=1, n_jobs=1, return_train_score=True)\n",
    "\n",
    "search.fit(X_train_fs, y_train)\n",
    "\n",
    "#report_best_scores(search.cv_results_, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e065f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "search.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5400b90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60865dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from stack\n",
    "clf_xgb = xgb.XGBClassifier(objective = 'binary:logistic')\n",
    "param_dist = {'n_estimators': stats.randint(150, 1000),\n",
    "              'learning_rate': stats.uniform(0.01, 0.59),\n",
    "              'subsample': stats.uniform(0.3, 0.6),\n",
    "              'max_depth': [3, 4, 5, 6, 7, 8, 9],\n",
    "              'colsample_bytree': stats.uniform(0.5, 0.4),\n",
    "              'min_child_weight': [1, 2, 3, 4]\n",
    "             }\n",
    "\n",
    "numFolds = 5\n",
    "kfold_5 = cross_validation.KFold(n = len(X), shuffle = True, n_folds = numFolds)\n",
    "\n",
    "clf = RandomizedSearchCV(clf_xgb, \n",
    "                         param_distributions = param_dist,\n",
    "                         cv = kfold_5,  \n",
    "                         n_iter = 5, # you want 5 here not 25 if I understand you correctly \n",
    "                         scoring = 'roc_auc', \n",
    "                         error_score = 0, \n",
    "                         verbose = 3, \n",
    "                         n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbdc5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "scores = []\n",
    "\n",
    "for train_index, test_index in kfold.split(X_train_fs):   \n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    xgb_model = xgb.XGBClassifier(objective=\"multi:softprob\", random_state=42)\n",
    "    xgb_model.fit(X_train_fs, y_train)\n",
    "    \n",
    "    y_pred = xgb_model.predict(X_test_fs)\n",
    "    \n",
    "    scores.append(mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "display_scores(np.sqrt(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9856f647",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = RandomForestClassifier()\n",
    "kfold = model_selection.KFold(n_splits=10)\n",
    "s = model_selection.cross_val_score(cls, X,y, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575e4e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_rf = RandomForestClassifier()\n",
    "kfold = model_selection.KFold(n_splits=10)\n",
    "cls_rf.fit(X_train, y_train)\n",
    "y_pred = cls_rf.predict(X_test)\n",
    "s = model_selection.cross_val_score(cls_rf, X_test,y_test, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07becbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056881af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(mypath+'/data/survey20_updated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8fd5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900b0112",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1[df1['JobSat'] !=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76c9a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6435fa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.JobSat.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b842412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data from the file\n",
    "#df = pd.read_csv(mypath+'/data/survey20_updated.csv')\n",
    "# preprocess, split and process data\n",
    "preproc_df1 = uf.preprocess_data(df1)\n",
    "X1_train, y1_train, X1_test, y1_test = uf.process_data(preproc_df1, 'JobSat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4736c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Always scale the input. The most convenient way is to use a pipeline.\n",
    "sgd_clf = make_pipeline(StandardScaler(),SGDClassifier(max_iter=1000, tol=1e-3))\n",
    "sgd_clf.fit(X1_train, y1_train)\n",
    "y1_pred = sgd_clf.predict(X1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ab63bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = confusion_matrix(y1_test, y1_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(result1)\n",
    "\n",
    "result2 = classification_report(y1_test, y1_pred, zero_division=0)\n",
    "print('\\nClassification Report:')\n",
    "print (result2)\n",
    "\n",
    "result3 = accuracy_score(y1_test,y1_pred)  \n",
    "print('Accuracy: %.3f' %result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93da4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a path string\n",
    "mypath = os.getcwd()\n",
    "\n",
    "df2 = pd.read_csv(mypath+'/data/survey20_updated.csv')\n",
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482491d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess, split and process data\n",
    "#preproc_df2 = uf.preprocess_data(df2)\n",
    "X2_train, y2_train, X2_test, y2_test = uf.process_data(preproc_df2, 'JobSat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18f87c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = pd.concat([X2_train, X2_test])\n",
    "y2 = pd.concat([y2_train, y2_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30eb1928",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c864feb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in [DecisionTreeClassifier, KNeighborsClassifier, GaussianNB, SVC, \n",
    "              RandomForestClassifier, SGDClassifier]:\n",
    "    make_pipeline(StandardScaler(),model())\n",
    "    classifier = model()\n",
    "    kfold = model_selection.KFold(n_splits=10)\n",
    "    classifier.fit(X2, y2)\n",
    "    s = model_selection.cross_val_score(classifier, X2,y2, cv=kfold)\n",
    "    #result2 = classification_report(y_test, y_pred, zero_division=0)\n",
    "    #s = model_selection.cross_val_score(cls, X, y, cv=kfold)\n",
    "    print(f\"{model.__name__:22}  CV_Mean:\" f\"{s.mean():.3f} CV_STD: {s.std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec7cd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Always scale the input. The most convenient way is to use a pipeline.\n",
    "sgd_clf = make_pipeline(StandardScaler(),SGDClassifier(max_iter=1000, tol=1e-3))\n",
    "sgd_clf.fit(X2_train, y2_train)\n",
    "y2_pred = sgd_clf.predict(X2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c07d63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = confusion_matrix(y2_test, y2_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(result1)\n",
    "\n",
    "result2 = classification_report(y2_test, y2_pred, zero_division=0)\n",
    "print('\\nClassification Report:')\n",
    "print (result2)\n",
    "\n",
    "result3 = accuracy_score(y2_test,y2_pred)  \n",
    "print('Accuracy: %.3f' %result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805a3165",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
