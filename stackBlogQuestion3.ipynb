{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7993e6b6",
   "metadata": {},
   "source": [
    "# Analysis of StackOverflow Survey. Part IV \n",
    "\n",
    "In this notebook we build a predictiv model for job satisfaction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "837206f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import neccessary packages and libraries\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# to render plots in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "# set a theme for seaborn\n",
    "sns.set_theme()\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn import (\n",
    "    ensemble,\n",
    "    preprocessing,\n",
    "    tree,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    StratifiedKFold,\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    r2_score, \n",
    "    mean_squared_error,\n",
    "    auc,\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed1cca52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import local module containing the neccessary functions\n",
    "import utils_functions as uf\n",
    "\n",
    "# forces the interpreter to re-import the module\n",
    "import importlib\n",
    "importlib.reload(uf);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc49bb0",
   "metadata": {},
   "source": [
    "## State the question\n",
    "I am addressing the third question in this notebook. What can we tell about the job satisfaction of a data coder? What factors do influence it? Also, predict the job satisfaction for a developer who works with big data. \n",
    "\n",
    "This is a classification question, we are predicting a satisfaction level for a data developer, which includes: data scientist or machine learning specialist, data or business analyst and data engineer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcfad12",
   "metadata": {},
   "source": [
    "## Performance metrics - to review at the end\n",
    "\n",
    "The following performance measures will be used in this project:\n",
    "1. Cross validation via StratifiedKFold with 10 folds.\n",
    "2. Confusion matrix, in particular precision, recall and F1 score.\n",
    "3. The ROC curve and the related AUC score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452d2a27",
   "metadata": {},
   "source": [
    "## Gather the data\n",
    "\n",
    "Upload the data and keep the subset that contains those developers that work in data science related fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fb9ad29e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64461, 25)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a path string\n",
    "mypath = os.getcwd()\n",
    "\n",
    "# upload the datafiles as pandas dataframes\n",
    "df1 = pd.read_csv(mypath+'/data/survey20_updated.csv')\n",
    "\n",
    "# check the uploaded data\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8354b521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8726, 25)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the data frame that contains the data developers only\n",
    "df1 = df1[df1.DevClass == 'data_coder']\n",
    "\n",
    "# check the size of the data\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "00aebee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of columns to be used in this analysis\n",
    "list_cols = ['MainBranch', 'ConvertedComp', \n",
    "       'EdLevel', 'Employment',\n",
    "       'JobSat', 'EdImpt',\n",
    "       'Learn', 'Overtime', 'OpSys', 'OrgSize', \n",
    "       'UndergradMajor', 'WorkWeekHrs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c81c52d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8726, 12)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the dataset that contains only the listed columns\n",
    "df1 = df1[list_cols]\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "065ec945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the index \n",
    "df1.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "61bed308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8726 entries, 0 to 8725\n",
      "Data columns (total 12 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   MainBranch      8699 non-null   object \n",
      " 1   ConvertedComp   5810 non-null   float64\n",
      " 2   EdLevel         8581 non-null   object \n",
      " 3   Employment      8726 non-null   object \n",
      " 4   JobSat          8726 non-null   int64  \n",
      " 5   EdImpt          8206 non-null   object \n",
      " 6   Learn           7979 non-null   object \n",
      " 7   Overtime        7424 non-null   object \n",
      " 8   OpSys           8120 non-null   object \n",
      " 9   OrgSize         7590 non-null   object \n",
      " 10  UndergradMajor  8053 non-null   object \n",
      " 11  WorkWeekHrs     6995 non-null   float64\n",
      "dtypes: float64(2), int64(1), object(9)\n",
      "memory usage: 818.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# gather information on dtypes and missing values\n",
    "df1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccdce8e",
   "metadata": {},
   "source": [
    "## Data profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6a33e150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this once to generate a profiling report and save it as html file\n",
    "\n",
    "#import pandas_profiling\n",
    "#profile = pandas_profiling.ProfileReport(df, minimal=False)\n",
    "#profile.to_file(output_file=\"data_coders_report.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6c4287",
   "metadata": {},
   "source": [
    "## Data preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad0c14a",
   "metadata": {},
   "source": [
    "### Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6c883ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8553, 12)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop duplicate rows, if any\n",
    "df1.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badc8282",
   "metadata": {},
   "source": [
    "### Create bins for the WorkWeekHrs column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f38e9f27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30-40    3842\n",
       "40-50    1836\n",
       ">50       610\n",
       "<10       284\n",
       "20-30     276\n",
       "10-20     140\n",
       "Name: WorkWeek_Bins, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the labels\n",
    "cut_labels = ['<10', '10-20', '20-30', '30-40', '40-50', '>50']\n",
    "\n",
    "# define the bins \n",
    "m = df1.WorkWeekHrs.max()\n",
    "cut_bins = [0, 10, 20, 30, 40, 50, m]\n",
    "\n",
    "# create a new column which contains the new labels\n",
    "df1['WorkWeek_Bins'] = pd.cut(df1['WorkWeekHrs'], bins=cut_bins, labels=cut_labels)\n",
    "\n",
    "# check for success\n",
    "df1['WorkWeek_Bins'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2d4bde5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1565"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the missing values in the new column\n",
    "df1['WorkWeek_Bins'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1dfe3260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the type of the newly created column\n",
    "df1['WorkWeek_Bins'] = df1['WorkWeek_Bins'].astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "622851d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the WorkWeekHrs column\n",
    "df1.drop(columns = 'WorkWeekHrs', inplace=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7360c337",
   "metadata": {},
   "source": [
    "### Create bins for the ConvertedComp column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f06f88ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we could use quantile, however I prefer custom bins here\n",
    "cut_labels = ['<10K', '10K-30K', '30K-50K', '50K-100K', '100K-200K', '>200K']\n",
    "\n",
    "# define the bins \n",
    "m = df1.ConvertedComp.max()\n",
    "cut_bins = [0, 10000, 30000, 50000, 100000, 200000, m]\n",
    "\n",
    "# create a new column which contains the new labels\n",
    "df1['Comp_Bins'] = pd.cut(df1['ConvertedComp'], bins=cut_bins, labels=cut_labels)\n",
    "\n",
    "# change the type of the newly created column\n",
    "df1['Comp_Bins'] = df1['Comp_Bins'].astype('object')\n",
    "\n",
    "# drop the WorkWeekHrs column\n",
    "df1.drop(columns = 'ConvertedComp', inplace=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7b99f9",
   "metadata": {},
   "source": [
    "## Create features and target\n",
    "\n",
    "Create a dataframe (X) with the features and a pandas series (y) that contains the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "59b40b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy of the pre-processed dataframe\n",
    "df2 = df1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fdf816df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8553 entries, 0 to 8725\n",
      "Data columns (total 11 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   MainBranch      8526 non-null   object\n",
      " 1   EdLevel         8410 non-null   object\n",
      " 2   Employment      8553 non-null   object\n",
      " 3   EdImpt          8144 non-null   object\n",
      " 4   Learn           7818 non-null   object\n",
      " 5   Overtime        7417 non-null   object\n",
      " 6   OpSys           7954 non-null   object\n",
      " 7   OrgSize         7580 non-null   object\n",
      " 8   UndergradMajor  7905 non-null   object\n",
      " 9   WorkWeek_Bins   6988 non-null   object\n",
      " 10  Comp_Bins       5783 non-null   object\n",
      "dtypes: object(11)\n",
      "memory usage: 801.8+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, 8553)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the predictors dataframe\n",
    "X = df2.drop(columns = 'JobSat')\n",
    "\n",
    "# create the labels\n",
    "y = df2['JobSat']\n",
    "\n",
    "# check for success\n",
    "X.info(), len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa411ff",
   "metadata": {},
   "source": [
    "## Create dummies for the dataframe of predictors X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0aaa49dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8553, 69)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dummies for all the columns in dataframe\n",
    "X_dumm = pd.get_dummies(X)\n",
    "\n",
    "# check for success\n",
    "X_dumm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a45ce76",
   "metadata": {},
   "source": [
    "## Sample data\n",
    "\n",
    "We will use $30 \\%$ data for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bf0d97b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5987, 69), 5987, (2566, 69), 2566)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split the data in train and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_dumm, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# check for success\n",
    "X_train.shape, len(y_train), X_test.shape, len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a115bc",
   "metadata": {},
   "source": [
    "## Impute missing values\n",
    "\n",
    "Now that we have test and train data, we can impute missing values on the training set, and use the trained imputer to fill in the test dataset. I will use the KNN imputer from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0d4945f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MainBranch_I am a developer by profession                                                   False\n",
       "MainBranch_I am a student who is learning to code                                           False\n",
       "MainBranch_I am not primarily a developer, but I write code sometimes as part of my work    False\n",
       "MainBranch_I code primarily as a hobby                                                      False\n",
       "MainBranch_I used to be a developer by profession, but no longer am                         False\n",
       "                                                                                            ...  \n",
       "Comp_Bins_10K-30K                                                                           False\n",
       "Comp_Bins_30K-50K                                                                           False\n",
       "Comp_Bins_50K-100K                                                                          False\n",
       "Comp_Bins_<10K                                                                              False\n",
       "Comp_Bins_>200K                                                                             False\n",
       "Length: 69, dtype: bool"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create an instance of the imputer\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "# fit the imputer on the dataset\n",
    "X_train_trans = pd.DataFrame(imputer.fit_transform(X_train), columns = X_train.columns)\n",
    "\n",
    "# check for success\n",
    "X_train_trans.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bd6ebe",
   "metadata": {},
   "source": [
    "## Refactor code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c06092e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a path string\n",
    "mypath = os.getcwd()\n",
    "\n",
    "# read the data from the file\n",
    "df = pd.read_csv(mypath+'/data/survey20_updated.csv')\n",
    "# preprocess, split and process data\n",
    "preproc_df = uf.preprocess_data(df)\n",
    "X_train, y_train, X_test, y_test = uf.process_data(preproc_df, 'JobSat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d47cc4",
   "metadata": {},
   "source": [
    "## Baseline model: K NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9934c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of the classifier\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=10)\n",
    "\n",
    "# fit the classifier\n",
    "knn_clf.fit(X_train, y_train)\n",
    "\n",
    "# predict output values\n",
    "y_pred = knn_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22c3bf63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[263   1   1   2   4   8]\n",
      " [  5   9  12   8  81  67]\n",
      " [ 11  14  47  21 144 119]\n",
      " [ 10  10  29  18 116 104]\n",
      " [ 11  13  75  34 287 249]\n",
      " [ 33  14  70  40 285 351]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.94      0.86       279\n",
      "           1       0.15      0.05      0.07       182\n",
      "           2       0.20      0.13      0.16       356\n",
      "           3       0.15      0.06      0.09       287\n",
      "           4       0.31      0.43      0.36       669\n",
      "           5       0.39      0.44      0.42       793\n",
      "\n",
      "    accuracy                           0.38      2566\n",
      "   macro avg       0.33      0.34      0.33      2566\n",
      "weighted avg       0.34      0.38      0.35      2566\n",
      "\n",
      "Accuracy: 0.380\n"
     ]
    }
   ],
   "source": [
    "# print evaluation metrics and results\n",
    "\n",
    "result1 = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(result1)\n",
    "\n",
    "result2 = classification_report(y_test, y_pred)\n",
    "print('\\nClassification Report:')\n",
    "print (result2)\n",
    "\n",
    "result3 = accuracy_score(y_test,y_pred)  \n",
    "print('Accuracy: %.3f' %result3)\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8cd09a",
   "metadata": {},
   "source": [
    "## Several other algorithms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f664a43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(gamma='auto', random_state=42)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# create classifier instance\n",
    "svm_clf = SVC(gamma=\"auto\", random_state=42)\n",
    "# fit the model\n",
    "svm_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5a0f5392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 4)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict on the test set\n",
    "y_pred = svm_clf.predict(X_test)\n",
    "\n",
    "# test one value\n",
    "y_test.iloc[20],  y_pred[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ae437bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.28494909,  0.74793612,  3.16192468,  1.90535375,  5.2713231 ,\n",
       "        4.2675518 ])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_digit_scores = svm_clf.decision_function(X_test)\n",
    "some_digit_scores[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4fd7f7f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(some_digit_scores[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0acae03d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "376bba3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "173c0f2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.39378758, 0.42084168, 0.41002506])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(svm_clf, X_train, y_train, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4e0d8b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[251   0   0   0  12  16]\n",
      " [  0   0   0   0  62 120]\n",
      " [  0   0   0   0 141 215]\n",
      " [  0   0   0   0 109 178]\n",
      " [  0   0   0   0 252 417]\n",
      " [  0   0   0   0 181 612]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.90      0.95       279\n",
      "           1       0.00      0.00      0.00       182\n",
      "           2       0.00      0.00      0.00       356\n",
      "           3       0.00      0.00      0.00       287\n",
      "           4       0.33      0.38      0.35       669\n",
      "           5       0.39      0.77      0.52       793\n",
      "\n",
      "    accuracy                           0.43      2566\n",
      "   macro avg       0.29      0.34      0.30      2566\n",
      "weighted avg       0.32      0.43      0.36      2566\n",
      "\n",
      "Accuracy: 0.435\n"
     ]
    }
   ],
   "source": [
    "# print evaluation metrics and results\n",
    "\n",
    "result1 = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(result1)\n",
    "\n",
    "result2 = classification_report(y_test, y_pred, zero_division=0)\n",
    "print('\\nClassification Report:')\n",
    "print (result2)\n",
    "\n",
    "result3 = accuracy_score(y_test,y_pred)  \n",
    "print('Accuracy: %.3f' %result3)\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6b45d132",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "ovr_clf = OneVsRestClassifier(SVC(gamma=\"auto\", random_state=42))\n",
    "ovr_clf.fit(X_train, y_train)\n",
    "y_pred = ovr_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ac945a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[272   2   1   1   0   3]\n",
      " [  5   5  15   9  43 105]\n",
      " [ 11  12  31  15 112 175]\n",
      " [ 15  10  29  16  85 132]\n",
      " [ 18  26  42  37 181 365]\n",
      " [ 32  33  44  28 174 482]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.97      0.86       279\n",
      "           1       0.06      0.03      0.04       182\n",
      "           2       0.19      0.09      0.12       356\n",
      "           3       0.15      0.06      0.08       287\n",
      "           4       0.30      0.27      0.29       669\n",
      "           5       0.38      0.61      0.47       793\n",
      "\n",
      "    accuracy                           0.38      2566\n",
      "   macro avg       0.31      0.34      0.31      2566\n",
      "weighted avg       0.33      0.38      0.34      2566\n",
      "\n",
      "Accuracy: 0.385\n"
     ]
    }
   ],
   "source": [
    "result1 = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(result1)\n",
    "\n",
    "result2 = classification_report(y_test, y_pred, zero_division=0)\n",
    "print('\\nClassification Report:')\n",
    "print (result2)\n",
    "\n",
    "result3 = accuracy_score(y_test,y_pred)  \n",
    "print('Accuracy: %.3f' %result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b4f0b71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Always scale the input. The most convenient way is to use a pipeline.\n",
    "sgd_clf = make_pipeline(StandardScaler(),SGDClassifier(max_iter=1000, tol=1e-3))\n",
    "sgd_clf.fit(X_train, y_train)\n",
    "y_pred = sgd_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3033a0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[265   2   3   3   6   0]\n",
      " [  0  17  17  19  56  73]\n",
      " [  1  19  46  27 135 128]\n",
      " [  0  23  32  27 101 104]\n",
      " [  1  54  67  66 239 242]\n",
      " [  4  47  70  79 256 337]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.96       279\n",
      "           1       0.10      0.09      0.10       182\n",
      "           2       0.20      0.13      0.16       356\n",
      "           3       0.12      0.09      0.11       287\n",
      "           4       0.30      0.36      0.33       669\n",
      "           5       0.38      0.42      0.40       793\n",
      "\n",
      "    accuracy                           0.36      2566\n",
      "   macro avg       0.35      0.34      0.34      2566\n",
      "weighted avg       0.35      0.36      0.35      2566\n",
      "\n",
      "Accuracy: 0.363\n"
     ]
    }
   ],
   "source": [
    "result1 = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(result1)\n",
    "\n",
    "result2 = classification_report(y_test, y_pred, zero_division=0)\n",
    "print('\\nClassification Report:')\n",
    "print (result2)\n",
    "\n",
    "result3 = accuracy_score(y_test,y_pred)  \n",
    "print('Accuracy: %.3f' %result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f4a03721",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sgd_clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-6ae79887c106>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX_train_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msgd_clf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sgd_clf' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.astype(np.float64))\n",
    "cross_val_score(sgd_clf, X_train_scaled, y_train, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "aed95eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([X_train, X_test])\n",
    "y = pd.concat([y_train, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c23bdba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import (LogisticRegression)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import (KNeighborsClassifier)\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import (RandomForestClassifier)\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "02d44922",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/silvia/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/silvia/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/silvia/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py\", line 726, in fit\n",
      "    missing=self.missing, nthread=self.n_jobs)\n",
      "  File \"/home/silvia/anaconda3/lib/python3.7/site-packages/xgboost/core.py\", line 426, in __init__\n",
      "    self.feature_names = feature_names\n",
      "  File \"/home/silvia/anaconda3/lib/python3.7/site-packages/xgboost/core.py\", line 870, in feature_names\n",
      "    raise ValueError('feature_names may not contain [, ] or <')\n",
      "ValueError: feature_names may not contain [, ] or <\n",
      "\n",
      "  FitFailedWarning)\n",
      "/home/silvia/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/silvia/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/silvia/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py\", line 726, in fit\n",
      "    missing=self.missing, nthread=self.n_jobs)\n",
      "  File \"/home/silvia/anaconda3/lib/python3.7/site-packages/xgboost/core.py\", line 426, in __init__\n",
      "    self.feature_names = feature_names\n",
      "  File \"/home/silvia/anaconda3/lib/python3.7/site-packages/xgboost/core.py\", line 870, in feature_names\n",
      "    raise ValueError('feature_names may not contain [, ] or <')\n",
      "ValueError: feature_names may not contain [, ] or <\n",
      "\n",
      "  FitFailedWarning)\n",
      "/home/silvia/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/silvia/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/silvia/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py\", line 726, in fit\n",
      "    missing=self.missing, nthread=self.n_jobs)\n",
      "  File \"/home/silvia/anaconda3/lib/python3.7/site-packages/xgboost/core.py\", line 426, in __init__\n",
      "    self.feature_names = feature_names\n",
      "  File \"/home/silvia/anaconda3/lib/python3.7/site-packages/xgboost/core.py\", line 870, in feature_names\n",
      "    raise ValueError('feature_names may not contain [, ] or <')\n",
      "ValueError: feature_names may not contain [, ] or <\n",
      "\n",
      "  FitFailedWarning)\n",
      "/home/silvia/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/silvia/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/silvia/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py\", line 726, in fit\n",
      "    missing=self.missing, nthread=self.n_jobs)\n",
      "  File \"/home/silvia/anaconda3/lib/python3.7/site-packages/xgboost/core.py\", line 426, in __init__\n",
      "    self.feature_names = feature_names\n",
      "  File \"/home/silvia/anaconda3/lib/python3.7/site-packages/xgboost/core.py\", line 870, in feature_names\n",
      "    raise ValueError('feature_names may not contain [, ] or <')\n",
      "ValueError: feature_names may not contain [, ] or <\n",
      "\n",
      "  FitFailedWarning)\n",
      "/home/silvia/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/silvia/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/silvia/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py\", line 726, in fit\n",
      "    missing=self.missing, nthread=self.n_jobs)\n",
      "  File \"/home/silvia/anaconda3/lib/python3.7/site-packages/xgboost/core.py\", line 426, in __init__\n",
      "    self.feature_names = feature_names\n",
      "  File \"/home/silvia/anaconda3/lib/python3.7/site-packages/xgboost/core.py\", line 870, in feature_names\n",
      "    raise ValueError('feature_names may not contain [, ] or <')\n",
      "ValueError: feature_names may not contain [, ] or <\n",
      "\n",
      "  FitFailedWarning)\n",
      "/home/silvia/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/silvia/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/silvia/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py\", line 726, in fit\n",
      "    missing=self.missing, nthread=self.n_jobs)\n",
      "  File \"/home/silvia/anaconda3/lib/python3.7/site-packages/xgboost/core.py\", line 426, in __init__\n",
      "    self.feature_names = feature_names\n",
      "  File \"/home/silvia/anaconda3/lib/python3.7/site-packages/xgboost/core.py\", line 870, in feature_names\n",
      "    raise ValueError('feature_names may not contain [, ] or <')\n",
      "ValueError: feature_names may not contain [, ] or <\n",
      "\n",
      "  FitFailedWarning)\n",
      "/home/silvia/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/silvia/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/silvia/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py\", line 726, in fit\n",
      "    missing=self.missing, nthread=self.n_jobs)\n",
      "  File \"/home/silvia/anaconda3/lib/python3.7/site-packages/xgboost/core.py\", line 426, in __init__\n",
      "    self.feature_names = feature_names\n",
      "  File \"/home/silvia/anaconda3/lib/python3.7/site-packages/xgboost/core.py\", line 870, in feature_names\n",
      "    raise ValueError('feature_names may not contain [, ] or <')\n",
      "ValueError: feature_names may not contain [, ] or <\n",
      "\n",
      "  FitFailedWarning)\n",
      "/home/silvia/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/silvia/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/silvia/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py\", line 726, in fit\n",
      "    missing=self.missing, nthread=self.n_jobs)\n",
      "  File \"/home/silvia/anaconda3/lib/python3.7/site-packages/xgboost/core.py\", line 426, in __init__\n",
      "    self.feature_names = feature_names\n",
      "  File \"/home/silvia/anaconda3/lib/python3.7/site-packages/xgboost/core.py\", line 870, in feature_names\n",
      "    raise ValueError('feature_names may not contain [, ] or <')\n",
      "ValueError: feature_names may not contain [, ] or <\n",
      "\n",
      "  FitFailedWarning)\n",
      "/home/silvia/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/silvia/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/silvia/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py\", line 726, in fit\n",
      "    missing=self.missing, nthread=self.n_jobs)\n",
      "  File \"/home/silvia/anaconda3/lib/python3.7/site-packages/xgboost/core.py\", line 426, in __init__\n",
      "    self.feature_names = feature_names\n",
      "  File \"/home/silvia/anaconda3/lib/python3.7/site-packages/xgboost/core.py\", line 870, in feature_names\n",
      "    raise ValueError('feature_names may not contain [, ] or <')\n",
      "ValueError: feature_names may not contain [, ] or <\n",
      "\n",
      "  FitFailedWarning)\n",
      "/home/silvia/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/silvia/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/silvia/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py\", line 726, in fit\n",
      "    missing=self.missing, nthread=self.n_jobs)\n",
      "  File \"/home/silvia/anaconda3/lib/python3.7/site-packages/xgboost/core.py\", line 426, in __init__\n",
      "    self.feature_names = feature_names\n",
      "  File \"/home/silvia/anaconda3/lib/python3.7/site-packages/xgboost/core.py\", line 870, in feature_names\n",
      "    raise ValueError('feature_names may not contain [, ] or <')\n",
      "ValueError: feature_names may not contain [, ] or <\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    }
   ],
   "source": [
    "for model in [DecisionTreeClassifier, KNeighborsClassifier, GaussianNB, SVC, \n",
    "              RandomForestClassifier, xgb.XGBClassifier]:\n",
    "    cls = model()\n",
    "    kfold = model_selection.KFold(n_splits=10)\n",
    "    #result2 = classification_report(y_test, y_pred, zero_division=0)\n",
    "    s = model_selection.cross_val_score(cls, X, y, cv=kfold)\n",
    "    #print(f\"{model.__name__:22}  AUC:\" f\"{s.mean():.3f} STD: {s.std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07becbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227b37d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = xgb.XGBClassifier()\n",
    "kfold = model_selection.KFold(n_splits=10)\n",
    "s = model_selection.cross_val_score(cls, X, y, scoring=\"roc_auc\", cv=kfold)\n",
    "    print(f\"{model.__name__:22}  AUC:\" f\"{s.mean():.3f} STD: {s.std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d90322",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = load_diabetes()\n",
    "\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "scores = []\n",
    "\n",
    "for train_index, test_index in kfold.split(X):   \n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    xgb_model = xgb.XGBRegressor(objective=\"reg:linear\")\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = xgb_model.predict(X_test)\n",
    "    \n",
    "    scores.append(mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "display_scores(np.sqrt(scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
