{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9449fd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general packages and libraries\n",
    "import os\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import importlib\n",
    "\n",
    "# data manipulation packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# data visualizations packages\n",
    "import matplotlib.pyplot as plt\n",
    "# to render plots in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "# set a theme for seaborn\n",
    "sns.set_theme()\n",
    "\n",
    "# numerical, statistical and machine learning packages and libraries\n",
    "import xgboost as xgb\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn import (\n",
    "    ensemble,\n",
    "    tree,\n",
    ")\n",
    "from sklearn.base import (\n",
    "    BaseEstimator, \n",
    "    TransformerMixin,\n",
    ")\n",
    "from sklearn.pipeline import (\n",
    "    make_pipeline,\n",
    "    FeatureUnion, \n",
    "    Pipeline,\n",
    ")\n",
    "from sklearn.feature_selection import (\n",
    "    SelectKBest, \n",
    "    chi2, \n",
    "    mutual_info_classif,\n",
    "    f_classif,\n",
    ")\n",
    "from sklearn.impute import (\n",
    "    KNNImputer,\n",
    "    SimpleImputer,\n",
    ")\n",
    "from sklearn.preprocessing import (\n",
    "    OneHotEncoder, \n",
    "    OrdinalEncoder, \n",
    "    LabelEncoder,\n",
    "    StandardScaler,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    StratifiedKFold,\n",
    "    KFold,\n",
    "    cross_val_score,\n",
    ")\n",
    "\n",
    "#from sklearn.dummy import DummyClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "#from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "#from sklearn.ensemble import (RandomForestClassifier)\n",
    "\n",
    "from sklearn.linear_model import (\n",
    "    SGDClassifier,\n",
    "    LogisticRegression,\n",
    ") \n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    r2_score, \n",
    "    mean_squared_error,\n",
    "    auc,\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7993e6b6",
   "metadata": {},
   "source": [
    "# Analysis of StackOverflow Survey\n",
    "\n",
    "This notebook contains notes, alternate solutions and references for the project StackOverflow Developers' Survey 2020. It mainly concerns the modeling part which builds a multi-class classifier for the job satisfaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e7c6ce",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb9ad29e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64461, 61)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a path string\n",
    "mypath = os.getcwd()\n",
    "\n",
    "# upload the datafiles as pandas dataframes\n",
    "df = pd.read_csv(mypath+'/data/survey20_updated.csv', index_col=[0])\n",
    "\n",
    "# check the uploaded data\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a65127c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy of the data\n",
    "dft = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7ca967",
   "metadata": {},
   "source": [
    "## Remove unecessary data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cd96da",
   "metadata": {},
   "source": [
    "### Keep the developers that work with data\n",
    "\n",
    "- The column DevType is a multiple strings column.\n",
    "- In the analysis we differentiate between data developers and other developers only.\n",
    "- Data developers are: data scientist or machine learning specialist, data or business analyst, data engineer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b3a27c",
   "metadata": {},
   "source": [
    "#### Method 1: create an auxiliary column DevClass\n",
    "\n",
    "- Create a new column that marks if the developer works with data or not.\n",
    "- Keep only those data points that correspond to data developers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bbd166f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these steps were performed in the initial stage of data preprocessing\n",
    "\n",
    "# rename the data engineer string in the full dataset\n",
    "#df['DevType'] = df['DevType'].str.replace('Engineer, data', 'Data engineer')\n",
    "\n",
    "# respondents choose more than one answer as we can see below\n",
    "# DevType_counts = df.DevType.value_counts().reset_index()\n",
    "\n",
    "# create a list of the individual answers that can be marked by a user\n",
    "#dev_choice = list(DevType_counts.devChoice.str.split(';', expand=True)[0].unique())\n",
    "\n",
    "# the list of types of developers working with data\n",
    "#data_dev = [x for x in dev_choice if 'Data ' in x]\n",
    "\n",
    "# use np.where(condition, value if condition is true, value if condition is false)\n",
    "# create column DevClass, entry data_coder or other_coder, based on DevType contains data or not\n",
    "#df1['DevClass'] = np.where(df1[\"DevType\"].str.contains(\"Data \", na = False), 'data_coder', 'other_coder')\n",
    "\n",
    "# the subset of developers that checked at least one data related profession\n",
    "#data_coders = df1[df1[\"DevClass\"] == 'data_coder']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0230acc6",
   "metadata": {},
   "source": [
    "#### Method 2: split the strings and use pandas explode\n",
    "\n",
    "- This creates rows duplicates, one for each choice of developer type.\n",
    "- I suspect this method induces data leakage and eventually overfitting. \n",
    "- This method splits the developers in three categories, as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11b19bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change each string in DevType column into a list of strings\n",
    "# this is necessary for pd.explode to work\n",
    "#df1['DevType'] = df1['DevType'].str.split(';')\n",
    "\n",
    "# split a row with multiple choices strings in DevType into rows where\n",
    "# DevType contains only one choice, the index is replicated \n",
    "#df1=df1.explode('DevType')\n",
    "\n",
    "# drop the rows with missing values in DevType column\n",
    "# necessary for the string search in the next step to work\n",
    "#df1.dropna(subset=['DevType'], inplace=True)\n",
    "\n",
    "# retain only those rows that contain data coders\n",
    "#df1 = df1[df1['DevType'].str.contains('Data ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14848633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep the developers that work with data, use the custom function\n",
    "\n",
    "# rewrite entries in 'DevType' column as strings to replicate rows\n",
    "dft = uf.explode_col(dft, 'DevType')\n",
    "\n",
    "# retain only those rows that contain data coders\n",
    "dft = dft.loc[dft.DevType.str.contains('Data ', na=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14be3ec",
   "metadata": {},
   "source": [
    "#### Method 3: split the strings in DevType and create corresponding columns\n",
    "\n",
    "- Split each string in the entries of DevType.\n",
    "- Create sparse column for each developer type.\n",
    "- Retain only those data points that correspond to one of the data developers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40227f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://github.com/adrianhasse/job-satisfaction\n",
    "\n",
    "def handle_multi_string_columns(df, column, single_strings):\n",
    "    '''\n",
    "    Replaces column whose fields contain several strings with new columns. Each\n",
    "    new column will then represent a single string\n",
    "    \n",
    "    INPUT:\n",
    "    df - the pandas dataframe you want to search\n",
    "    column - the column name you want to look through\n",
    "    single_strings - a list of strings you want to search for in each row of df[col]\n",
    "\n",
    "    OUTPUT:\n",
    "    new_df - The dataframe without the multi-string column but with the newly created columns\n",
    "    col_dict - Dictionary translating names of the new columns to their corresponding string\n",
    "    '''\n",
    "    \n",
    "    #collects new columns of indicating if a certain index refers to a string \n",
    "    new_columns = dict()\n",
    "    \n",
    "    #dict column name -> string name\n",
    "    col_dict = dict()\n",
    "    \n",
    "    #loop through list of strings\n",
    "    counter = 0\n",
    "    for string in single_strings:\n",
    "        bool_list = []\n",
    "        #loop through rows\n",
    "        for idx in range(df.shape[0]):\n",
    "            #if the ed type is in the row set to True\n",
    "            if string in str(df[column][idx]):\n",
    "                bool_list.append(1)\n",
    "            else:\n",
    "                bool_list.append(0)\n",
    "        col_name = column + \"_\" + str(counter)\n",
    "        new_columns[col_name] = bool_list\n",
    "        col_dict[col_name] = string\n",
    "        counter = counter + 1\n",
    "    \n",
    "    new_df = df.drop(column,axis=1)\n",
    "    \n",
    "    new_df = pd.concat([new_df, pd.DataFrame(data=new_columns, index = df.index, dtype=int)], axis=1)\n",
    "    \n",
    "    return new_df, col_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "378287a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StringtoSetTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Scikit-learn transformer to convert the feature which is a string with fields separated by comma into a column\n",
    "    of a list\"\"\"\n",
    "    def __init__(self, variables=None):\n",
    "        if not isinstance(variables, list):\n",
    "            self.variables = [variables]\n",
    "        else:\n",
    "            self.variables = variables\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for feature in self.variables:\n",
    "            # if nan, create an empty set instead of imputation\n",
    "            X[feature] = X[feature].str.split(';').apply(lambda x: {} if\n",
    "                                                         x is np.nan else set(x))\n",
    "        return X\n",
    "    \n",
    "\n",
    "class ListColumnsEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Scikit-learn transformer to convert a feature column of a list in \n",
    "    to multiple binary feature columns\"\"\"\n",
    "    def __init__(self, variables=None):\n",
    "        \n",
    "        if not isinstance(variables, list):\n",
    "            self.variables = [variables]\n",
    "        else:\n",
    "            self.variables = variables\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # persist mode in a dictionary\n",
    "        self.encoder_dict_ = {}\n",
    "        \n",
    "        for feature in self.variables:\n",
    "            _ = MultiLabelBinarizer()\n",
    "            _.fit(X[feature])\n",
    "            self.encoder_dict_[feature] = _\n",
    "\n",
    "    def transform(self, X):\n",
    "\n",
    "        X = X.copy()\n",
    "        for feature in self.variables:\n",
    "            f_encoded = pd.DataFrame(\n",
    "                self.encoder_dict_[feature].transform(X[feature]),\n",
    "                columns=self.encoder_dict_[feature].classes_,\n",
    "                index=X.index)\n",
    "\n",
    "            X = pd.concat([X, f_encoded], axis=1).drop(columns=[feature])\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce1783a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MY APPROACH WITHOUT CLASSES\n",
    "\n",
    "# replace the list of entries with sets, missing values with empy set\n",
    "df1['PlatformWorkedWith'] = df1['PlatformWorkedWith'].str.split(';').apply(lambda x: {} if\n",
    "\n",
    "# check the outcome                                                                      x is np.nan else set(x))\n",
    "df1['PlatformWorkedWith'][:6]\n",
    "\n",
    "# create an instance of the encoder\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "# fit the binarizer and encode the selected column\n",
    "temp_col = mlb.fit_transform(df1['PlatformWorkedWith'])\n",
    "\n",
    "# put the outcome in pandas dataframe form\n",
    "temp_df = pd.DataFrame(temp_col, columns=mlb.classes_, index=df1.index)\n",
    "\n",
    "# combine the two dataframes and drop the initial column\n",
    "df1 = pd.concat([df1, temp_df], axis=1).drop(columns = ['PlatformWorkedWith'])\n",
    "\n",
    "# check the outcome\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cf7b63",
   "metadata": {},
   "source": [
    "## Feature selection\n",
    "\n",
    "Various ways to perform feature selection. Not included in the final approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a6125b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/Chancylin/StackOverflow_Survey/blob/main/code/data_process.py\n",
    "\n",
    "def cal_mutual_info(df, target_var=None, disc_features_only=True):\n",
    "    \"\"\"Calculate mutual information for feature selection, based on mutual_info_classif from sklearn.feature_selection.\n",
    "    :param df: Pandas dataframe\n",
    "    :param target_var: target variable\n",
    "    :param disc_features_only: boolean, calculate mutual information for discrete feature only\n",
    "    :return:\n",
    "        a Pandas dataframe with mutual information for features\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    df_f_type = df.dtypes\n",
    "    df_f_type = df_f_type.loc[~df_f_type.index.isin([target_var])].copy()\n",
    "    cols_if_num = df_f_type.apply(lambda x: np.issubdtype(x, np.number))\n",
    "    discrete_f = ~cols_if_num\n",
    "    # get all categorical features\n",
    "    cols_num = cols_if_num[cols_if_num].index.tolist()\n",
    "    cols_cat = cols_if_num[~cols_if_num].index.tolist()\n",
    "\n",
    "    for col_cat in cols_cat:\n",
    "        df[col_cat] = df[col_cat].fillna('Missing')\n",
    "\n",
    "    for col_num in cols_num:\n",
    "        df[col_num] = df[col_num].fillna(df[col_num].mean())\n",
    "        \n",
    "    enc = OrdinalEncoder()\n",
    "    df[cols_cat] = enc.fit_transform(df[cols_cat])\n",
    "    enc = OrdinalEncoder()\n",
    "    df.loc[:, target_var] = enc.fit_transform(df[[target_var]])\n",
    "\n",
    "    if not disc_features_only:\n",
    "        all_features = df_f_type.index.tolist()\n",
    "        mutual_info = mutual_info_classif(df[all_features], df[target_var].values,\n",
    "                                          discrete_features=discrete_f,\n",
    "                                          n_neighbors=20,\n",
    "                                          random_state=123)\n",
    "        df_mutual_info = pd.DataFrame(data=zip(all_features, mutual_info),\n",
    "                                      columns=['columns', 'mutual_info'])\n",
    "        return df_mutual_info\n",
    "    else:\n",
    "\n",
    "        mutual_info = mutual_info_classif(df[cols_cat], df[target_var].values,\n",
    "                                          discrete_features=True)\n",
    "        df_mutual_info = pd.DataFrame(data=zip(cols_cat, mutual_info),\n",
    "                                      columns=['columns', 'mutual_info'])\n",
    "        return df_mutual_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4af4a0",
   "metadata": {},
   "source": [
    "### My approach: step_by_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "719e41b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Age', 'CompTotal', 'ConvertedComp', 'WorkWeekHrs']\n"
     ]
    }
   ],
   "source": [
    "# the list of numerical columns\n",
    "num_cols = df1.select_dtypes(include='float64').columns.to_list()\n",
    "print(num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55f91054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MainBranch', 'Hobbyist', 'Age1stCode', 'CompFreq', 'Country', 'CurrencyDesc', 'CurrencySymbol', 'DatabaseDesireNextYear', 'DatabaseWorkedWith', 'DevType', 'EdLevel', 'Employment', 'Ethnicity', 'Gender', 'JobFactors', 'JobSat', 'JobSeek', 'LanguageDesireNextYear', 'LanguageWorkedWith', 'MiscTechDesireNextYear', 'MiscTechWorkedWith', 'CollabToolsDesireNextYear', 'CollabToolsWorkedWith', 'DevOps', 'DevOpsImpt', 'EdImpt', 'JobHunt', 'JobHuntResearch', 'Learn', 'OffTopic', 'OnboardGood', 'OtherComms', 'Overtime', 'PurchaseResearch', 'PurpleLink', 'SOSites', 'Stuck', 'OpSys', 'OrgSize', 'PlatformDesireNextYear', 'PlatformWorkedWith', 'PurchaseWhat', 'Sexuality', 'SOAccount', 'SOComm', 'SOPartFreq', 'SOVisitFreq', 'SurveyEase', 'SurveyLength', 'Trans', 'UndergradMajor', 'WebframeDesireNextYear', 'WebframeWorkedWith', 'WelcomeChange', 'YearsCode', 'YearsCodePro', 'DevClass']\n"
     ]
    }
   ],
   "source": [
    "# the list of categorical columns, drop the target 'JobSat'\n",
    "cat_cols = df1.select_dtypes(include='object').columns.to_list()\n",
    "print(cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cca2664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the list of columns with high cardinality - will get a high selection score\n",
    "multiple = ['DatabaseDesireNextYear', 'DatabaseWorkedWith', \n",
    "           'LanguageDesireNextYear', 'LanguageWorkedWith',\n",
    "            'MiscTechDesireNextYear', 'MiscTechWorkedWith', \n",
    "            'CollabToolsDesireNextYear', 'CollabToolsWorkedWith', \n",
    "            'PlatformDesireNextYear', 'PlatformWorkedWith', \n",
    "            'WebframeDesireNextYear', 'WebframeWorkedWith']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad532941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OnboardGood', 'WelcomeChange', 'Ethnicity', 'JobHunt', 'OffTopic', 'SOPartFreq', 'Country', 'SOComm', 'MainBranch', 'SurveyLength', 'CompFreq', 'YearsCode', 'DevClass', 'Gender', 'JobHuntResearch', 'DevOps', 'Overtime', 'OtherComms', 'Age1stCode', 'EdImpt', 'YearsCodePro', 'SOVisitFreq', 'Trans', 'Learn', 'DevType', 'SOSites', 'Sexuality', 'PurpleLink', 'CurrencyDesc', 'SurveyEase', 'PurchaseResearch', 'JobFactors', 'Employment', 'Stuck', 'CurrencySymbol', 'OpSys', 'Hobbyist', 'OrgSize', 'UndergradMajor', 'SOAccount', 'EdLevel', 'JobSeek', 'PurchaseWhat', 'DevOpsImpt']\n"
     ]
    }
   ],
   "source": [
    "# remove target 'JobSat' and the multiple columns from cat_cols\n",
    "cat_cols = list(set(cat_cols) - set(multiple))\n",
    "cat_cols.remove('JobSat')\n",
    "print(cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8354b521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop all missing entries in 'JobSat'\n",
    "df1.dropna(subset=['JobSat'], inplace=True)\n",
    "# check output\n",
    "df1['JobSat'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3af85dd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MainBranch          0\n",
       "Hobbyist            0\n",
       "Age              8839\n",
       "Age1stCode          0\n",
       "CompFreq            0\n",
       "                 ... \n",
       "WelcomeChange       0\n",
       "WorkWeekHrs      4137\n",
       "YearsCode           0\n",
       "YearsCodePro        0\n",
       "DevClass            0\n",
       "Length: 61, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fill in 'missing' in all categorical columns in the list\n",
    "for col in cat_cols:\n",
    "        df1[col] = df1[col].fillna('missing')\n",
    "# check outcome\n",
    "df1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00aebee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age              0\n",
       "CompTotal        0\n",
       "ConvertedComp    0\n",
       "WorkWeekHrs      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fill in 'median' in all numerical columns in the list\n",
    "for col in num_cols:\n",
    "        df1[col] = df1[col].fillna(df1[col].median())\n",
    "# check outcome\n",
    "df1[num_cols].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "128e8053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the numerical columns\n",
    "\n",
    "# create an instance of the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# scale the numerical variables, fit and transform on the straining set\n",
    "df1[num_cols] = pd.DataFrame(scaler.fit_transform(df1[num_cols]), \n",
    "                                columns=df1[num_cols].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6234f9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the categorical variables\n",
    "enc = OrdinalEncoder()\n",
    "df1[cat_cols] = enc.fit_transform(df1[cat_cols])\n",
    "enc = OrdinalEncoder()\n",
    "df1.loc[:, 'JobSat'] = enc.fit_transform(df1[['JobSat']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d21265f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection function, to be rewritten as a custom transformer class\n",
    "def select_features(X_train, y_train, score_function, kval, no_cols):# add X_test\n",
    "    \"\"\"\n",
    "    Function for feature selection for (discrete) variables.\n",
    "    INPUT: X_train - input dataframe, must have only discrete or only continuous features, \n",
    "                     pre-processed by removing/imputing missing values and encoded\n",
    "           y_train - target pd.series, pre-processed\n",
    "           score_function - can be any of the score functions supported by SelectKBest\n",
    "           kval = the number of best features to return, can be 'all'\n",
    "           no_cols = number of columns to print\n",
    "    OUTPUT: dataframe with two columns, one for the column names in X_train, \n",
    "            the second for the scores computed, sorted in decreasing order of the scores\n",
    "            #variant: if transform of X_test set is performed, \n",
    "                      it also returns the transformed dataframe\n",
    "    \"\"\"\n",
    "    # create an instance of the selector\n",
    "    fs = SelectKBest(score_func=score_function, k=kval)\n",
    "    # fit the selector on the train set and the train target values\n",
    "    fs.fit(X_train, y_train)\n",
    "    # transform the train set, it will have only the kbest columns\n",
    "    X_train_r = fs.transform(X_train) \n",
    "    # transform the test set, it will have only the kbest columns\n",
    "    #X_test_r = fs.transform(X_test) \n",
    "    # get column names for kbest columns\n",
    "    cols_info = fs.get_support(indices=True)\n",
    "    cols = X_train.iloc[:,cols_info].columns\n",
    "    # put columns and their scores together in a dataframe\n",
    "    frame_best = pd.DataFrame(data=zip(cols,fs.scores_), columns = ['cat_columns', 'kbest_scores'])\n",
    "    return frame_best.sort_values(by='kbest_scores', ascending=False).head(no_cols)\n",
    "    return X_r, frame_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40007dd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_columns</th>\n",
       "      <th>kbest_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>JobSeek</td>\n",
       "      <td>0.101638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OnboardGood</td>\n",
       "      <td>0.030806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JobHunt</td>\n",
       "      <td>0.030312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Country</td>\n",
       "      <td>0.026406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>CurrencyDesc</td>\n",
       "      <td>0.016943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>CurrencySymbol</td>\n",
       "      <td>0.015994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>PurchaseWhat</td>\n",
       "      <td>0.013272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ethnicity</td>\n",
       "      <td>0.009715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Age1stCode</td>\n",
       "      <td>0.009144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>OtherComms</td>\n",
       "      <td>0.008300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CompFreq</td>\n",
       "      <td>0.007640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Stuck</td>\n",
       "      <td>0.007559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>DevType</td>\n",
       "      <td>0.007554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>DevOps</td>\n",
       "      <td>0.006551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>SOSites</td>\n",
       "      <td>0.006465</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cat_columns  kbest_scores\n",
       "41         JobSeek      0.101638\n",
       "0      OnboardGood      0.030806\n",
       "3          JobHunt      0.030312\n",
       "6          Country      0.026406\n",
       "28    CurrencyDesc      0.016943\n",
       "34  CurrencySymbol      0.015994\n",
       "42    PurchaseWhat      0.013272\n",
       "2        Ethnicity      0.009715\n",
       "18      Age1stCode      0.009144\n",
       "17      OtherComms      0.008300\n",
       "10        CompFreq      0.007640\n",
       "33           Stuck      0.007559\n",
       "24         DevType      0.007554\n",
       "15          DevOps      0.006551\n",
       "25         SOSites      0.006465"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the above function - done without X_test\n",
    "select_features(df1[cat_cols], df1['JobSat'], mutual_info_classif, 'all', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cbeadca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternate approach to feature selection, using mutual_info_classif\n",
    "mutual_info = mutual_info_classif(df1[cat_cols], df1['JobSat'],\n",
    "                                          discrete_features='auto',\n",
    "                                          n_neighbors=3,\n",
    "                                          copy=True,\n",
    "                                          random_state=42)\n",
    "df_mutual_info = pd.DataFrame(data=zip(cat_cols, mutual_info), columns=['columns', 'mutual_info'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "038822d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>columns</th>\n",
       "      <th>mutual_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>JobSeek</td>\n",
       "      <td>0.100693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JobHunt</td>\n",
       "      <td>0.031575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OnboardGood</td>\n",
       "      <td>0.030981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>CurrencyDesc</td>\n",
       "      <td>0.020989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>CurrencySymbol</td>\n",
       "      <td>0.020525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Country</td>\n",
       "      <td>0.016402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>JobFactors</td>\n",
       "      <td>0.010464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>PurchaseResearch</td>\n",
       "      <td>0.010394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ethnicity</td>\n",
       "      <td>0.010051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>PurchaseWhat</td>\n",
       "      <td>0.008045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>JobHuntResearch</td>\n",
       "      <td>0.007736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CompFreq</td>\n",
       "      <td>0.007328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>DevOpsImpt</td>\n",
       "      <td>0.006611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Overtime</td>\n",
       "      <td>0.006337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>YearsCodePro</td>\n",
       "      <td>0.006122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             columns  mutual_info\n",
       "41           JobSeek     0.100693\n",
       "3            JobHunt     0.031575\n",
       "0        OnboardGood     0.030981\n",
       "28      CurrencyDesc     0.020989\n",
       "34    CurrencySymbol     0.020525\n",
       "6            Country     0.016402\n",
       "31        JobFactors     0.010464\n",
       "30  PurchaseResearch     0.010394\n",
       "2          Ethnicity     0.010051\n",
       "42      PurchaseWhat     0.008045\n",
       "14   JobHuntResearch     0.007736\n",
       "10          CompFreq     0.007328\n",
       "43        DevOpsImpt     0.006611\n",
       "16          Overtime     0.006337\n",
       "20      YearsCodePro     0.006122"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mutual_info.sort_values(by='mutual_info', ascending=False)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96aeab0e",
   "metadata": {},
   "source": [
    "## Binning continuous variables\n",
    "\n",
    "Tried in the first version. The accuracy scores of the models are very low. Did not include in the final approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badc8282",
   "metadata": {},
   "source": [
    "### Create bins for the WorkWeekHrs column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38e9f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy of the data\n",
    "df1 = df.copy()\n",
    "\n",
    "# create the labels\n",
    "cut_labels = ['less-10', '10-20', '20-30', '30-40', '40-50', 'more-50']\n",
    "\n",
    "# define the bins \n",
    "m = df1.WorkWeekHrs.max()\n",
    "cut_bins = [0, 10, 20, 30, 40, 50, m]\n",
    "\n",
    "# create a new column which contains the new labels\n",
    "df1['WorkWeek_Bins'] = pd.cut(df1['WorkWeekHrs'], bins=cut_bins, labels=cut_labels)\n",
    "\n",
    "# check for success\n",
    "df1['WorkWeek_Bins'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfe3260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the type of the newly created column\n",
    "df1['WorkWeek_Bins'] = df1['WorkWeek_Bins'].astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622851d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the WorkWeekHrs column\n",
    "df1.drop(columns = 'WorkWeekHrs', inplace=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7360c337",
   "metadata": {},
   "source": [
    "### Create bins for the ConvertedComp column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06f88ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we could use quantile, however I prefer custom bins here\n",
    "cut_labels = ['less-10K', '10K-30K', '30K-50K', '50K-100K', '100K-200K', 'more-200K']\n",
    "\n",
    "# define the bins \n",
    "m = df1.ConvertedComp.max()\n",
    "cut_bins = [0, 10000, 30000, 50000, 100000, 200000, m]\n",
    "\n",
    "# create a new column which contains the new labels\n",
    "df1['Comp_Bins'] = pd.cut(df1['ConvertedComp'], bins=cut_bins, labels=cut_labels)\n",
    "\n",
    "# change the type of the newly created column\n",
    "df1['Comp_Bins'] = df1['Comp_Bins'].astype('object')\n",
    "\n",
    "# drop the WorkWeekHrs column\n",
    "df1.drop(columns = 'ConvertedComp', inplace=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f30ade",
   "metadata": {},
   "source": [
    "### Create bins for the Age column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6297d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the bin edges\n",
    "cut_labels = ['<20', '20-30', '30-40', '40-50', '50-60', '60-70', '70-80', '>80']\n",
    "\n",
    "# define the bins \n",
    "m = df1.Age.max()\n",
    "cut_bins = [0, 20, 30, 40, 50, 60, 70, 80, m]\n",
    "\n",
    "# create a new column which contains the new labels\n",
    "df1['Age_Bins'] = pd.cut(df1['Age'], bins=cut_bins, labels=cut_labels)\n",
    "\n",
    "# change the type of the newly created column\n",
    "df1['Age_Bins'] = df1['Age_Bins'].astype('object')\n",
    "\n",
    "# drop the WorkWeekHrs column\n",
    "df1.drop(columns = 'Age', inplace=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff817994",
   "metadata": {},
   "source": [
    "### Remove the rows that contain mostly missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e323f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the rows with at least 10 non-NA values\n",
    "df1.dropna(thresh=10)\n",
    "\n",
    "# check the result\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1298d714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b00d9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c7b99f9",
   "metadata": {},
   "source": [
    "## Create features and target\n",
    "\n",
    "Create a dataframe (X) with the features and a pandas series (y) that contains the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf816df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy of the pre-processed dataframe\n",
    "df2 = df1.copy()\n",
    "\n",
    "# create the predictors dataframe\n",
    "X = df2.drop(columns = 'JobSat')\n",
    "\n",
    "# create the labels\n",
    "y = df2['JobSat']\n",
    "\n",
    "# check for success\n",
    "print(X.info(), len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c049e8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# format all the fields as strings in the feature matrix\n",
    "X = X.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a45ce76",
   "metadata": {},
   "source": [
    "## Sample data\n",
    "\n",
    "We will use $30 \\%$ data for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0d97b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data in train and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# summarize the data\n",
    "print('Train', X_train.shape, y_train.shape)\n",
    "print('Test', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a115bc",
   "metadata": {},
   "source": [
    "## Impute missing values\n",
    "\n",
    "Now that we have test and train data, we can impute missing values on the training set, and use the trained imputer to fill in the test dataset. I will use the KNN imputer from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4945f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of the imputer\n",
    "#imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "# fit the imputer on the dataset\n",
    "#X_train_trans = pd.DataFrame(imputer.fit_transform(X_train), columns = X_train.columns)\n",
    "\n",
    "# check for success\n",
    "#X_train_trans.isna().any()\n",
    "from sklearn.impute import SimpleImputer\n",
    "def impute_predictors(X_train, X_test):\n",
    "    imputer = SimpleImputer(strategy='constant', fill_value='missing')\n",
    "    imputer.fit(X_train)\n",
    "    X_train_trans = pd.DataFrame(imputer.transform(X_train), columns=X_train.columns)\n",
    "    X_test_trans = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)\n",
    "    return X_train_trans, X_test_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483501ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_trans, X_test_trans = impute_predictors(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4962e5b1",
   "metadata": {},
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, [0])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c2de48",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_trans.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa411ff",
   "metadata": {},
   "source": [
    "## Encode the data\n",
    "\n",
    "The best practice when encoding variables is to fit the encoding on the training dataset, then apply it to the train and test datasets.\n",
    "\n",
    "The function below named prepare_inputs() takes the input data for the train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16463bad",
   "metadata": {},
   "outputs": [],
   "source": [
    " # prepare input data\n",
    "def encode_predictors(X_train, X_test):\n",
    "\tenc = OneHotEncoder(handle_unknown='ignore')\n",
    "\tenc.fit(X_train)\n",
    "\tX_train_enc = enc.transform(X_train)\n",
    "\tX_test_enc = enc.transform(X_test)\n",
    "\treturn X_train_enc, X_test_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96179add",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_enc, X_test_enc = encode_predictors(X_train_trans, X_test_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d614018",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19108de7",
   "metadata": {},
   "source": [
    "Regarding the output data, the target, since it is already encoded as an integer with values from 0 to 5, no other encoding steps are needed at this point.\n",
    "\n",
    "Alternative would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c6ae2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare target\n",
    "#def encode_targets(y_train, y_test):\n",
    "\t#le = LabelEncoder()\n",
    "\t#le.fit(y_train)\n",
    "\t#y_train_enc = le.transform(y_train)\n",
    "\t#y_test_enc = le.transform(y_test)\n",
    "\t#return y_train_enc, y_test_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97eb6d1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0bd6ebe",
   "metadata": {},
   "source": [
    "## Refactor code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17aef62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c06092e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a path string\n",
    "mypath = os.getcwd()\n",
    "\n",
    "# read the data from the file\n",
    "df = pd.read_csv(mypath+'/data/survey20_updated.csv')\n",
    "# preprocess, split and process data\n",
    "preproc_df = uf.preprocess_data(df)\n",
    "X_train, y_train, X_test, y_test = uf.process_data(preproc_df, 'JobSat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d47cc4",
   "metadata": {},
   "source": [
    "## Baseline model: K NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9934c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of the classifier\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# fit the classifier\n",
    "knn_clf.fit(X_train_fs, y_train)\n",
    "\n",
    "# predict output values\n",
    "y_pred = knn_clf.predict(X_test_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c3bf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print evaluation metrics and results\n",
    "\n",
    "result1 = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(result1)\n",
    "\n",
    "result2 = classification_report(y_test, y_pred)\n",
    "print('\\nClassification Report:')\n",
    "print (result2)\n",
    "\n",
    "result3 = accuracy_score(y_test,y_pred)  \n",
    "print('Accuracy: %.3f' %result3)\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8cd09a",
   "metadata": {},
   "source": [
    "## Several other algorithms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f664a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# create classifier instance\n",
    "svm_clf = SVC(gamma=\"auto\", random_state=42)\n",
    "# fit the model\n",
    "svm_clf.fit(X_train_fs, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0f5392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on the test set\n",
    "y_pred = svm_clf.predict(X_test_fs)\n",
    "\n",
    "# test one value\n",
    "y_test.iloc[20],  y_pred[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae437bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_digit_scores = svm_clf.decision_function(X_test_fs)\n",
    "some_digit_scores[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd7f7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(some_digit_scores[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acae03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376bba3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173c0f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(svm_clf, X_test_fs, y_test, cv=10, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0d8b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print evaluation metrics and results\n",
    "\n",
    "result1 = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(result1)\n",
    "\n",
    "result2 = classification_report(y_test, y_pred, zero_division=0)\n",
    "print('\\nClassification Report:')\n",
    "print (result2)\n",
    "\n",
    "result3 = accuracy_score(y_test,y_pred)  \n",
    "print('Accuracy: %.3f' %result3)\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b45d132",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "ovr_clf = OneVsRestClassifier(SVC(gamma=\"auto\", random_state=42))\n",
    "ovr_clf.fit(X_train_fs, y_train)\n",
    "y_pred = ovr_clf.predict(X_test_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac945a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(result1)\n",
    "\n",
    "result2 = classification_report(y_test, y_pred, zero_division=0)\n",
    "print('\\nClassification Report:')\n",
    "print (result2)\n",
    "\n",
    "result3 = accuracy_score(y_test,y_pred)  \n",
    "print('Accuracy: %.3f' %result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f0b71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Always scale the input. The most convenient way is to use a pipeline.\n",
    "sgd_clf = make_pipeline(StandardScaler(with_mean=False),SGDClassifier(max_iter=1000, tol=1e-3))\n",
    "sgd_clf.fit(X_train_fs, y_train)\n",
    "y_pred = sgd_clf.predict(X_test_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3033a0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(result1)\n",
    "\n",
    "result2 = classification_report(y_test, y_pred, zero_division=0)\n",
    "print('\\nClassification Report:')\n",
    "print (result2)\n",
    "\n",
    "result3 = accuracy_score(y_test,y_pred)  \n",
    "print('Accuracy: %.3f' %result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed95eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([X_train_fs, X_test_fs])\n",
    "y = pd.concat([y_train, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23bdba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import (LogisticRegression)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import (KNeighborsClassifier)\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import (RandomForestClassifier)\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d44922",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in [DecisionTreeClassifier, KNeighborsClassifier, GaussianNB, SVC, \n",
    "              RandomForestClassifier, SGDClassifier]:\n",
    "    make_pipeline(StandardScaler(),model())\n",
    "    classifier = model()\n",
    "    kfold = model_selection.KFold(n_splits=10)\n",
    "    classifier.fit(X_train_fs.toarray(), y_train)\n",
    "    s = model_selection.cross_val_score(classifier, X_test_fs.toarray(),y_test, cv=kfold)\n",
    "    #result2 = classification_report(y_test, y_pred, zero_division=0)\n",
    "    #s = model_selection.cross_val_score(cls, X, y, cv=kfold)\n",
    "    print(f\"{model.__name__:22}  CV_Mean:\" f\"{s.mean():.3f} CV_STD: {s.std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91249c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier(objective=\"multi:softprob\", random_state=42)\n",
    "xgb_model.fit(X_train_fs, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e1ab8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb_model.predict(X_test_fs)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ba9767",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6423fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer, load_diabetes, load_wine\n",
    "from sklearn.metrics import auc, accuracy_score, confusion_matrix, mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold, RandomizedSearchCV, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633392f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier(objective=\"multi:softprob\", random_state=42)\n",
    "\n",
    "params = {\n",
    "    \"colsample_bytree\": uniform(0.7, 0.3),\n",
    "    \"gamma\": uniform(0, 0.5),\n",
    "    \"learning_rate\": uniform(0.03, 0.3), # default 0.1 \n",
    "    \"max_depth\": randint(2, 6), # default 3\n",
    "    \"n_estimators\": randint(100, 150), # default 100\n",
    "    \"subsample\": uniform(0.6, 0.4)\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(xgb_model, param_distributions=params, random_state=42, \n",
    "                            n_iter=200, cv=3, verbose=1, n_jobs=1, return_train_score=True)\n",
    "\n",
    "search.fit(X_train_fs, y_train)\n",
    "\n",
    "#report_best_scores(search.cv_results_, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e065f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "search.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5400b90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60865dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from stack\n",
    "clf_xgb = xgb.XGBClassifier(objective = 'binary:logistic')\n",
    "param_dist = {'n_estimators': stats.randint(150, 1000),\n",
    "              'learning_rate': stats.uniform(0.01, 0.59),\n",
    "              'subsample': stats.uniform(0.3, 0.6),\n",
    "              'max_depth': [3, 4, 5, 6, 7, 8, 9],\n",
    "              'colsample_bytree': stats.uniform(0.5, 0.4),\n",
    "              'min_child_weight': [1, 2, 3, 4]\n",
    "             }\n",
    "\n",
    "numFolds = 5\n",
    "kfold_5 = cross_validation.KFold(n = len(X), shuffle = True, n_folds = numFolds)\n",
    "\n",
    "clf = RandomizedSearchCV(clf_xgb, \n",
    "                         param_distributions = param_dist,\n",
    "                         cv = kfold_5,  \n",
    "                         n_iter = 5, # you want 5 here not 25 if I understand you correctly \n",
    "                         scoring = 'roc_auc', \n",
    "                         error_score = 0, \n",
    "                         verbose = 3, \n",
    "                         n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbdc5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "scores = []\n",
    "\n",
    "for train_index, test_index in kfold.split(X_train_fs):   \n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    xgb_model = xgb.XGBClassifier(objective=\"multi:softprob\", random_state=42)\n",
    "    xgb_model.fit(X_train_fs, y_train)\n",
    "    \n",
    "    y_pred = xgb_model.predict(X_test_fs)\n",
    "    \n",
    "    scores.append(mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "display_scores(np.sqrt(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9856f647",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = RandomForestClassifier()\n",
    "kfold = model_selection.KFold(n_splits=10)\n",
    "s = model_selection.cross_val_score(cls, X,y, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575e4e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_rf = RandomForestClassifier()\n",
    "kfold = model_selection.KFold(n_splits=10)\n",
    "cls_rf.fit(X_train, y_train)\n",
    "y_pred = cls_rf.predict(X_test)\n",
    "s = model_selection.cross_val_score(cls_rf, X_test,y_test, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07becbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056881af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(mypath+'/data/survey20_updated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8fd5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900b0112",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1[df1['JobSat'] !=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76c9a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6435fa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.JobSat.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b842412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data from the file\n",
    "#df = pd.read_csv(mypath+'/data/survey20_updated.csv')\n",
    "# preprocess, split and process data\n",
    "preproc_df1 = uf.preprocess_data(df1)\n",
    "X1_train, y1_train, X1_test, y1_test = uf.process_data(preproc_df1, 'JobSat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4736c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Always scale the input. The most convenient way is to use a pipeline.\n",
    "sgd_clf = make_pipeline(StandardScaler(),SGDClassifier(max_iter=1000, tol=1e-3))\n",
    "sgd_clf.fit(X1_train, y1_train)\n",
    "y1_pred = sgd_clf.predict(X1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ab63bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = confusion_matrix(y1_test, y1_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(result1)\n",
    "\n",
    "result2 = classification_report(y1_test, y1_pred, zero_division=0)\n",
    "print('\\nClassification Report:')\n",
    "print (result2)\n",
    "\n",
    "result3 = accuracy_score(y1_test,y1_pred)  \n",
    "print('Accuracy: %.3f' %result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93da4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a path string\n",
    "mypath = os.getcwd()\n",
    "\n",
    "df2 = pd.read_csv(mypath+'/data/survey20_updated.csv')\n",
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482491d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess, split and process data\n",
    "#preproc_df2 = uf.preprocess_data(df2)\n",
    "X2_train, y2_train, X2_test, y2_test = uf.process_data(preproc_df2, 'JobSat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18f87c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = pd.concat([X2_train, X2_test])\n",
    "y2 = pd.concat([y2_train, y2_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30eb1928",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c864feb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in [DecisionTreeClassifier, KNeighborsClassifier, GaussianNB, SVC, \n",
    "              RandomForestClassifier, SGDClassifier]:\n",
    "    make_pipeline(StandardScaler(),model())\n",
    "    classifier = model()\n",
    "    kfold = model_selection.KFold(n_splits=10)\n",
    "    classifier.fit(X2, y2)\n",
    "    s = model_selection.cross_val_score(classifier, X2,y2, cv=kfold)\n",
    "    #result2 = classification_report(y_test, y_pred, zero_division=0)\n",
    "    #s = model_selection.cross_val_score(cls, X, y, cv=kfold)\n",
    "    print(f\"{model.__name__:22}  CV_Mean:\" f\"{s.mean():.3f} CV_STD: {s.std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec7cd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Always scale the input. The most convenient way is to use a pipeline.\n",
    "sgd_clf = make_pipeline(StandardScaler(),SGDClassifier(max_iter=1000, tol=1e-3))\n",
    "sgd_clf.fit(X2_train, y2_train)\n",
    "y2_pred = sgd_clf.predict(X2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c07d63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = confusion_matrix(y2_test, y2_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(result1)\n",
    "\n",
    "result2 = classification_report(y2_test, y2_pred, zero_division=0)\n",
    "print('\\nClassification Report:')\n",
    "print (result2)\n",
    "\n",
    "result3 = accuracy_score(y2_test,y2_pred)  \n",
    "print('Accuracy: %.3f' %result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805a3165",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
